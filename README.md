Download Link: https://assignmentchef.com/product/solved-cmput466-assignment-3
<br>
<strong>Problem 1</strong>

Give a closed-form solution to the loss

<strong>Problem 2</strong>

function value guarantees to decrease. In practice, we may annealα &gt; 0      α α, meaning that we start In the gradient descent algorithm,    is the learning rate. If  is small enough, then the from a relatively large α, but decrease it gradually. the gradient descent algorithm may not converge to the optimum of a convex function.α α Show that     cannot be decreased too fast. If         is decreased too fast, even if it is strictly positive,

<em>Hint</em>: Show a concrete loss and an annealing scheduler such that the gradient descent algorithm fails to converge to the optimum.

<em>Another Hint</em>: Think of the schema of our attendance bonus in this course. Why can’t a student get more than five marks even if the student catches infinite errors?

END of W3